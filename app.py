# 1. IMPORTS
from fastapi import FastAPI, Request, HTTPException, Depends
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os, uvicorn, logging, re, random  # Added random here
from datetime import datetime
import redis
from typing import List, Dict, Optional
import numpy as np
import json

# 2. LOGGING SETUP
if not os.path.exists('logs'):
    os.makedirs('logs')

logging.basicConfig(
    filename=f'logs/chatbot_{datetime.now().strftime("%Y%m%d")}.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# 3. APP INITIALIZATION
app = FastAPI(title="Chatbot API")

# 4. MIDDLEWARE SETUP
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"],
)

# 5. REDIS SETUP
redis_client = redis.from_url("redis://localhost", encoding="utf-8", decode_responses=True)
FastAPILimiter.init(redis_client)

# 6. DEVICE & MODEL CONFIGURATION
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è ƒêang s·ª≠ d·ª•ng: {device}")

MODEL_DIR = "training/results/best_model"

# 7. MODEL LOADING
try:
    print("ƒêang t·∫£i model...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_DIR,
        local_files_only=True
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(
        MODEL_DIR,
        local_files_only=True,
        use_cache=False
    )
    model = model.to(device)
    model.eval()
    print("ƒê√£ t·∫£i model th√†nh c√¥ng!")
except Exception as e:
    print(f"L·ªói khi t·∫£i model: {str(e)}")
    raise RuntimeError(f"Kh√¥ng th·ªÉ t·∫£i model: {str(e)}")

# 8. STATIC FILES SETUP
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# 9. UTILITY CLASSES AND FUNCTIONS
class ChatInput(BaseModel):
    user_input: str
    conversation_history: list = []  # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i

def clean_special_tokens(text):
    text = re.sub(r'<extra_id_\d+>', '', text)
    special_tokens = ['<pad>', '<s>', '</s>', '<unk>']
    for token in special_tokens:
        text = text.replace(token, '')
    return text.strip()

def format_conversation_context(history, current_input):
    """T·∫°o ng·ªØ c·∫£nh t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i"""
    context = ""
    # L·∫•y 3 l∆∞·ª£t h·ªôi tho·∫°i g·∫ßn nh·∫•t ƒë·ªÉ t·∫°o ng·ªØ c·∫£nh
    for turn in history[-3:]:
        context += f"Ng∆∞·ªùi d√πng: {turn['user']}\nBot: {turn['bot']}\n"
    context += f"Ng∆∞·ªùi d√πng: {current_input}\n"
    return context

# Th√™m h√†m ki·ªÉm tra c√¢u tr·∫£ l·ªùi c√≥ ph·∫£i l√† l·∫∑p l·∫°i c√¢u h·ªèi kh√¥ng
def is_repetitive_answer(question, answer):
    """Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c√≥ ph·∫£i l√† l·∫∑p l·∫°i c√¢u h·ªèi kh√¥ng"""
    # Chu·∫©n h√≥a c√¢u ƒë·ªÉ so s√°nh
    norm_question = question.lower().strip()
    norm_answer = answer.lower().strip()
    
    # Lo·∫°i b·ªè c√°c ti·ªÅn t·ªë "h·ªèi:" v√† "tr·∫£ l·ªùi:" ƒë·ªÉ so s√°nh
    norm_question = norm_question.replace("h·ªèi:", "").strip()
    norm_answer = norm_answer.replace("tr·∫£ l·ªùi:", "").strip()
    
    # Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng
    return norm_answer in norm_question or norm_question in norm_answer

# Th√™m danh s√°ch c√°c c√¢u tr·∫£ l·ªùi thay th·∫ø
FALLBACK_RESPONSES = [
    "Xin l·ªói, t√¥i ch∆∞a hi·ªÉu r√µ c√¢u h·ªèi c·ªßa b·∫°n. B·∫°n c√≥ th·ªÉ di·ªÖn ƒë·∫°t kh√°c ƒë∆∞·ª£c kh√¥ng?",
    "T√¥i kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ c√¢u tr·∫£ l·ªùi. B·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch r√µ h∆°n ƒë∆∞·ª£c kh√¥ng?",
    "C√¢u h·ªèi c·ªßa b·∫°n kh√° th√∫ v·ªã, nh∆∞ng t√¥i c·∫ßn th√™m th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c.",
    "B·∫°n c√≥ th·ªÉ cung c·∫•p th√™m chi ti·∫øt v·ªÅ v·∫•n ƒë·ªÅ n√†y ƒë∆∞·ª£c kh√¥ng?",
    "T√¥i ƒëang g·∫∑p kh√≥ khƒÉn trong vi·ªác hi·ªÉu c√¢u h·ªèi. B·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi theo c√°ch kh√°c kh√¥ng?"
]

# Th√™m c√°c h√†m utility m·ªõi
def get_intent(text: str) -> str:
    """Ph√¢n t√≠ch √Ω ƒë·ªãnh c·ªßa c√¢u h·ªèi"""
    text = text.lower()
    
    # ƒê·ªãnh nghƒ©a c√°c pattern c∆° b·∫£n
    intents = {
        'greeting': ['xin ch√†o', 'hello', 'hi', 'ch√†o'],
        'farewell': ['t·∫°m bi·ªát', 'bye', 'goodbye'],
        'question': ['l√† g√¨', 'nh∆∞ th·∫ø n√†o', 'bao gi·ªù', 'th·∫ø n√†o'],
        'thanks': ['c·∫£m ∆°n', 'thanks', 'thank you'],
    }
    
    for intent, patterns in intents.items():
        if any(p in text for p in patterns):
            return intent
    return 'other'

def enhance_context(history: List[Dict], current_input: str) -> str:
    """T·∫°o ng·ªØ c·∫£nh n√¢ng cao cho c√¢u h·ªèi"""
    context = ""
    
    # Th√™m th·ªùi gian
    current_hour = datetime.now().hour
    time_context = "bu·ªïi s√°ng" if 5 <= current_hour < 12 else \
                   "bu·ªïi chi·ªÅu" if 12 <= current_hour < 18 else "bu·ªïi t·ªëi"
    context += f"Th·ªùi ƒëi·ªÉm: {time_context}\n"
    
    # Ph√¢n t√≠ch √Ω ƒë·ªãnh
    intent = get_intent(current_input)
    context += f"√ù ƒë·ªãnh: {intent}\n"
    
    # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i
    if history:
        last_exchange = history[-1]
        context += f"C√¢u h·ªèi tr∆∞·ªõc: {last_exchange['user']}\n"
        context += f"C√¢u tr·∫£ l·ªùi tr∆∞·ªõc: {last_exchange['bot']}\n"
    
    # Th√™m c√¢u h·ªèi hi·ªán t·∫°i
    context += f"C√¢u h·ªèi hi·ªán t·∫°i: {current_input}\n"
    
    return context

# 10. API ENDPOINTS
@app.post("/chat")
async def chat_response(chat_input: ChatInput):
    user_text = chat_input.user_input.strip()
    if not user_text:
        raise HTTPException(status_code=400, detail="Vui l√≤ng nh·∫≠p c√¢u h·ªèi")

    try:
        # Get intent first
        intent = get_intent(user_text)
        
        # Handle simple intents immediately
        if intent == 'greeting':
            response = "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
            return {
                "response": response,
                "conversation_history": chat_input.conversation_history + [
                    {"user": user_text, "bot": response}
                ],
                "intent": intent
            }
            
        # Format input simpler
        input_text = f"H·ªèi: {user_text}"
        logging.info(f"üìù Processing input: {input_text}")
        
        # Tokenize input
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256
        ).to(device)

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=128,
                min_length=10,
                num_beams=5,
                no_repeat_ngram_size=2,
                repetition_penalty=1.5,
                length_penalty=1.0,
                early_stopping=True,
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9
            )

        # Process response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = clean_special_tokens(response)
        response = response.strip()

        # Check response quality
        if not response or len(response) < 5 or is_repetitive_answer(user_text, response):
            if intent == 'farewell':
                response = "T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i b·∫°n sau nh√©!"
            elif intent == 'thanks':
                response = "Kh√¥ng c√≥ g√¨! R·∫•t vui ƒë∆∞·ª£c gi√∫p b·∫°n."
            else:
                response = random.choice(FALLBACK_RESPONSES)
        
        # Format response
        response = response.capitalize()
        if not response.endswith(('.', '!', '?')):
            response += '.'

        # Update history
        updated_history = chat_input.conversation_history + [
            {"user": user_text, "bot": response}
        ][-5:]

        return {
            "response": response,
            "conversation_history": updated_history,
            "intent": intent
        }

    except Exception as e:
        logging.error(f"Error in chat_response: {str(e)}")
        return {
            "response": "Xin l·ªói, t√¥i ƒëang g·∫∑p s·ª± c·ªë. B·∫°n vui l√≤ng th·ª≠ l·∫°i sau nh√©!",
            "conversation_history": chat_input.conversation_history,
            "intent": "error"
        }

@app.get("/", response_class=HTMLResponse)
async def serve_homepage(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model": MODEL_DIR,
        "device": str(device),
        "memory": torch.cuda.memory_allocated() if torch.cuda.is_available() else "N/A"
    }

# 11. APP STARTUP
if __name__ == "__main__":
    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8000, 
        log_level="info"
    )
