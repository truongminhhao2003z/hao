# 1. IMPORTS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderCache, get_linear_schedule_with_warmup
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import json, re, os, warnings, logging
from datetime import datetime

# 2. LOGGING CONFIGURATION : ghi láº¡i thÃ´ng tin trong quÃ¡ trÃ¬nh cháº¡y chÆ°Æ¡ng trÃ¬nh  theo dÃµi training
log_dir = "training/logs"  #táº¡o thÆ° má»¥c
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler() #In thÃ´ng tin log ra console ngay khi cháº¡y
    ]
)

# 3. PREPROCESSING FUNCTIONS  lÃ m sáº¡ch vÄƒn báº£n Ä‘áº§u vÃ o trÆ°á»›c khi Ä‘Æ°a vÃ o model
def preprocess_input(text):
    """Xá»­ lÃ½ text Ä‘áº§u vÃ o: lowercase vÃ  loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t"""
    text = text.strip().lower()
    #strip(): Loáº¡i bá» khoáº£ng tráº¯ng Ä‘áº§u/cuá»‘i chuá»—i.
    #lower(): Chuyá»ƒn táº¥t cáº£ chá»¯ cÃ¡i thÃ nh chá»¯ thÆ°á»ng
    text = re.sub(r'[^\w\sÃ Ã¡áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»©á»«á»­á»¯á»±á»³Ã½á»·á»¹á»µÄ‘]', '', text)
    return text
#GiÃºp vÄƒn báº£n Ä‘á»“ng nháº¥t vÃ  sáº¡ch hÆ¡n, dá»… há»c hÆ¡n cho mÃ´ hÃ¬nh.
# 4. DATASET CLASS
class ChatbotDataset(Dataset):
    """Custom Dataset cho chatbot"""
    def __init__(self, data, tokenizer, max_length=128): #HÃ m khá»Ÿi táº¡o
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data) #Tráº£ vá» tá»•ng sá»‘ lÆ°á»£ng máº«u trong dataset.

    def __getitem__(self, idx): # 
        item = self.data[idx]
        input_text = f"chat: {item['input']}"
        target_text = item['output']
 # Láº¥y cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i tá»« dá»¯ liá»‡u, Ä‘á»‹nh dáº¡ng cÃ¢u há»i vá»›i prefix "chat: " Ä‘á»ƒ mÃ´ hÃ¬nh hiá»ƒu rÃµ má»¥c Ä‘Ã­ch Ä‘áº§u vÃ o.


        # Tokenize input vÃ  target
        #Token hÃ³a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra
        #Token hÃ³a lÃ  quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh cÃ¡c token (Ä‘Æ¡n vá»‹ ngá»¯ nghÄ©a nhá» hÆ¡n) mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.
        input_encodings = self.tokenizer(
            input_text, 
            max_length=self.max_length, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )
        target_encodings = self.tokenizer(
            target_text, 
            max_length=self.max_length, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )

        # Xá»­ lÃ½ encodings
        input_ids = input_encodings['input_ids'].squeeze() #ID cá»§a token Ä‘áº§u vÃ o.
        attention_mask = input_encodings['attention_mask'].squeeze() #mask giÃºp model phÃ¢n biá»‡t token tháº­t vÃ  padding.
        labels = target_encodings['input_ids'].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100 # Äáº·t giÃ¡ trá»‹ padding thÃ nh -100 Ä‘á»ƒ khÃ´ng tÃ­nh vÃ o loss.

        return { #Tráº£ vá» má»™t tá»« Ä‘iá»ƒn chá»©a cÃ¡c thÃ´ng tin cáº§n thiáº¿t cho mÃ´ hÃ¬nh.
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

 def evaluate_model(model, dataloader, device):
    """ÄÃ¡nh giÃ¡ model trÃªn validation set"""
    #TÃ³m táº¯t nhanh:
# HÃ m nÃ y cháº¡y model trÃªn táº­p validation, tÃ­nh average loss Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xem mÃ´ hÃ¬nh há»c tá»‘t Ä‘áº¿n Ä‘Ã¢u sau má»—i epoch.
# DÃ¹ng torch.no_grad() Ä‘á»ƒ táº¯t viá»‡c tÃ­nh gradient â€” ráº¥t cáº§n thiáº¿t khi khÃ´ng training.
# DÃ¹ng .eval() Ä‘á»ƒ Ä‘áº£m báº£o model hoáº¡t Ä‘á»™ng Ä‘Ãºng cháº¿ Ä‘á»™ inference.
    
    # Äáº·t model vá» cháº¿ Ä‘á»™ evaluation (khÃ´ng dÃ¹ng dropout, batchnorm,...)
    model.eval()

    total_val_loss = 0  # Biáº¿n dÃ¹ng Ä‘á»ƒ cá»™ng dá»“n loss trong quÃ¡ trÃ¬nh Ä‘Ã¡nh giÃ¡

    # KhÃ´ng tÃ­nh toÃ¡n gradient trong khi Ä‘Ã¡nh giÃ¡ â†’ tiáº¿t kiá»‡m bá»™ nhá»› vÃ  tÄƒng tá»‘c
    with torch.no_grad():
        for batch in dataloader:
            # ÄÆ°a dá»¯ liá»‡u batch lÃªn GPU (hoáº·c CPU tÃ¹y thiáº¿t bá»‹)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass: tÃ­nh toÃ¡n output vÃ  loss
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            
            # Cá»™ng dá»“n loss vÃ o tá»•ng loss
            total_val_loss += outputs.loss.item()
    
    # Tráº£ vá» loss trung bÃ¬nh trÃªn toÃ n bá»™ táº­p validation
    return total_val_loss / len(dataloader)

def main():
    # 6.1 Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Kiá»ƒm tra xem GPU cÃ³ sáºµn khÃ´ng, náº¿u khÃ´ng dÃ¹ng CPU
    print(f"\nğŸ–¥ï¸ Äang sá»­ dá»¥ng: {device}")
    
    # Hyperparameters
    BATCH_SIZE = 2  # Sá»‘ lÆ°á»£ng máº«u trong má»—i batch
    MAX_LENGTH = 64  # Äá»™ dÃ i tá»‘i Ä‘a cá»§a chuá»—i Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra
    NUM_EPOCHS = 5  # Sá»‘ epoch huáº¥n luyá»‡n
    
    # Directories
    cache_dir = "model_cache"  # ThÆ° má»¥c cache model Ä‘Ã£ táº£i
    results_dir = "training/results"  # ThÆ° má»¥c lÆ°u káº¿t quáº£
    os.makedirs(results_dir, exist_ok=True)  # Táº¡o thÆ° má»¥c káº¿t quáº£ náº¿u chÆ°a cÃ³

    # 6.2 Model Loading
    print("\n1ï¸âƒ£ Äang táº£i model tá»« cache...")
    try:
        # Táº£i tokenizer vÃ  model tá»« cache
        tokenizer = AutoTokenizer.from_pretrained(f"{cache_dir}/tokenizer", local_files_only=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            f"{cache_dir}/model",  # Táº£i model Ä‘Ã£ huáº¥n luyá»‡n tá»« cache
            local_files_only=True,
            use_cache=False  # Táº¯t sá»­ dá»¥ng cache khi táº£i model
        ).to(device)  # ÄÆ°a model lÃªn thiáº¿t bá»‹ (GPU/CPU)
        print("âœ… Táº£i model thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")
        return

    # 6.3 Data Loading
    print("\n2ï¸âƒ£ Äang táº£i dá»¯ liá»‡u...")
    try:
        # Táº£i dá»¯ liá»‡u huáº¥n luyá»‡n tá»« file JSON
        with open('training/data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… ÄÃ£ táº£i {len(data)} máº«u dá»¯ liá»‡u")
    except FileNotFoundError:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file data.json!")
        return

    # 6.4 Data Preparation
    print("\n3ï¸âƒ£ Chuáº©n bá»‹ dá»¯ liá»‡u...")
    # Táº¡o dataset tá»« dá»¯ liá»‡u Ä‘Ã£ táº£i vÃ  tokenizer
    dataset = ChatbotDataset(data, tokenizer, max_length=MAX_LENGTH)
    # Chia táº­p dá»¯ liá»‡u thÃ nh train vÃ  validation
    train_size = int(0.9 * len(dataset))  # 90% cho training
    val_size = len(dataset) - train_size  # 10% cho validation
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # Dataloader cho train vÃ  validation
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE,  # Sá»‘ lÆ°á»£ng máº«u trong má»—i batch
        shuffle=True,  # Shuffle dá»¯ liá»‡u
        num_workers=2,  # Sá»­ dá»¥ng 2 worker Ä‘á»ƒ load dá»¯ liá»‡u song song
        pin_memory=True  # Dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c lÆ°u trong bá»™ nhá»› pin Ä‘á»ƒ truy cáº­p nhanh hÆ¡n
    )
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        pin_memory=True
    )

    # 6.5 Optimizer & Scheduler Setup
    # Khá»Ÿi táº¡o optimizer vÃ  learning rate scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # AdamW optimizer
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=50,  # Sá»‘ bÆ°á»›c warmup
        num_training_steps=len(train_dataloader) * NUM_EPOCHS  # Tá»•ng sá»‘ bÆ°á»›c huáº¥n luyá»‡n
    )

    # 6.6 Training Loop
    print(f"\n4ï¸âƒ£ Báº¯t Ä‘áº§u training...")
    print(f"ğŸ“Š Train: {len(train_dataset)} máº«u, Validation: {len(val_dataset)} máº«u")
    print(f"ğŸ“ˆ Batch size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}")

    best_val_loss = float('inf')  # Khá»Ÿi táº¡o giÃ¡ trá»‹ loss tá»‘i Æ°u ban Ä‘áº§u
    for epoch in range(NUM_EPOCHS):
        # Training phase
        model.train()  # Chuyá»ƒn model sang cháº¿ Ä‘á»™ training
        total_loss = 0  # Khá»Ÿi táº¡o tá»•ng loss
        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')
        
        for batch in progress_bar:
            # Forward pass (tÃ­nh output tá»« batch)
            outputs = model(
                input_ids=batch['input_ids'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device)
            )
            
            # TÃ­nh loss vÃ  lÃ m backpropagation
            loss = outputs.loss
            total_loss += loss.item()  # Cá»™ng dá»“n loss
            loss.backward()  # Backpropagation

            # Optimization step (cáº­p nháº­t tham sá»‘ cá»§a model)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()  # Cáº­p nháº­t tham sá»‘
            scheduler.step()  # Cáº­p nháº­t learning rate
            optimizer.zero_grad()  # Reset gradient

            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})  # Hiá»ƒn thá»‹ loss trong progress bar

        # Validation phase
        val_loss = evaluate_model(model, val_dataloader, device)  # ÄÃ¡nh giÃ¡ loss trÃªn táº­p validation
        print(f"\nğŸ“Š Epoch {epoch+1}: Train Loss = {total_loss/len(train_dataloader):.4f}, Val Loss = {val_loss:.4f}")

        # Save best model based on validation loss
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            print("ğŸ’¾ LÆ°u model tá»‘t nháº¥t...")
            model.save_pretrained(f"{results_dir}/best_model")
            tokenizer.save_pretrained(f"{results_dir}/best_model")

    # 6.7 Model Testing
    print("\n5ï¸âƒ£ Test model...")
    test_inputs = [
        "Xin chÃ o!",  # Test case 1
        "2 + 2 báº±ng máº¥y?",  # Test case 2
        "HÃ´m nay thá»i tiáº¿t tháº¿ nÃ o?"  # Test case 3
    ]

    model.eval()  # Chuyá»ƒn model sang cháº¿ Ä‘á»™ inference
    for test_input in test_inputs:
        print(f"\nInput: {test_input}")
        processed_input = preprocess_input(test_input)  # Tiá»n xá»­ lÃ½ Ä‘áº§u vÃ o
        inputs = tokenizer(
            f"chat: {processed_input}", 
            return_tensors="pt",  # Chuyá»ƒn Ä‘á»•i thÃ nh tensor PyTorch
            max_length=64,  # Äáº·t Ä‘á»™ dÃ i tá»‘i Ä‘a cho chuá»—i
            padding=True,  # Padding náº¿u cáº§n
            truncation=True  # Cáº¯t bá»›t náº¿u quÃ¡ Ä‘á»™ dÃ i
        ).to(device)

        # Sinh cÃ¢u tráº£ lá»i tá»« mÃ´ hÃ¬nh
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=64,  # Äá»™ dÃ i tá»‘i Ä‘a cho cÃ¢u tráº£ lá»i
                num_beams=3,  # Beam search vá»›i 3 beam
                early_stopping=True  # Dá»«ng sá»›m khi Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c cÃ¢u tráº£ lá»i tá»‘t
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Giáº£i mÃ£ cÃ¢u tráº£ lá»i
        print(f"Output: {response}")

    print("\nâœ¨ Training hoÃ n táº¥t!")

# 7. ENTRY POINT
if __name__ == "__main__":
    main()
