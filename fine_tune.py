# 1. IMPORTS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderCache, get_linear_schedule_with_warmup
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from tqdm import tqdm
import json, re, os, warnings, logging
from datetime import datetime

# 2. LOGGING CONFIGURATION : ghi l·∫°i th√¥ng tin trong qu√° tr√¨nh ch·∫°y ch∆∞∆°ng tr√¨nh  theo d√µi training
log_dir = "training/logs"  #t·∫°o th∆∞ m·ª•c
os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'{log_dir}/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler() #In th√¥ng tin log ra console ngay khi ch·∫°y
    ]
)

# 3. PREPROCESSING FUNCTIONS  l√†m s·∫°ch vƒÉn b·∫£n ƒë·∫ßu v√†o tr∆∞·ªõc khi ƒë∆∞a v√†o model
def preprocess_input(text):
    """X·ª≠ l√Ω text ƒë·∫ßu v√†o: lowercase v√† lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát"""
    text = text.strip().lower()
    #strip(): Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi chu·ªói.
    #lower(): Chuy·ªÉn t·∫•t c·∫£ ch·ªØ c√°i th√†nh ch·ªØ th∆∞·ªùng
    text = re.sub(r'[^\w\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', '', text)
    return text
#Gi√∫p vƒÉn b·∫£n ƒë·ªìng nh·∫•t v√† s·∫°ch h∆°n, d·ªÖ h·ªçc h∆°n cho m√¥ h√¨nh.
# 4. DATASET CLASS
class ChatbotDataset(Dataset):
    """Custom Dataset cho chatbot"""
    def __init__(self, data, tokenizer, max_length=128): #H√†m kh·ªüi t·∫°o
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data) #Tr·∫£ v·ªÅ t·ªïng s·ªë l∆∞·ª£ng m·∫´u trong dataset.

    def __getitem__(self, idx): # 
        item = self.data[idx]
        input_text = f"chat: {item['input']}"
        target_text = item['output']
 # L·∫•y c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t·ª´ d·ªØ li·ªáu, ƒë·ªãnh d·∫°ng c√¢u h·ªèi v·ªõi prefix "chat: " ƒë·ªÉ m√¥ h√¨nh hi·ªÉu r√µ m·ª•c ƒë√≠ch ƒë·∫ßu v√†o.


        # Tokenize input v√† target
        #Token h√≥a ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
        #Token h√≥a l√† qu√° tr√¨nh chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh c√°c token (ƒë∆°n v·ªã ng·ªØ nghƒ©a nh·ªè h∆°n) m√† m√¥ h√¨nh c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.
        input_encodings = self.tokenizer(
            input_text, 
            max_length=self.max_length, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )
        target_encodings = self.tokenizer(
            target_text, 
            max_length=self.max_length, 
            padding='max_length', 
            truncation=True, 
            return_tensors='pt'
        )

        # X·ª≠ l√Ω encodings
        input_ids = input_encodings['input_ids'].squeeze() #ID c·ªßa token ƒë·∫ßu v√†o.
        attention_mask = input_encodings['attention_mask'].squeeze() #mask gi√∫p model ph√¢n bi·ªát token th·∫≠t v√† padding.
        labels = target_encodings['input_ids'].squeeze()
        labels[labels == self.tokenizer.pad_token_id] = -100 # ƒê·∫∑t gi√° tr·ªã padding th√†nh -100 ƒë·ªÉ kh√¥ng t√≠nh v√†o loss.

        return { #Tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn ch·ª©a c√°c th√¥ng tin c·∫ßn thi·∫øt cho m√¥ h√¨nh.
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

 def evaluate_model(model, dataloader, device):
    """ƒê√°nh gi√° model tr√™n validation set"""
    #T√≥m t·∫Øt nhanh:
# H√†m n√†y ch·∫°y model tr√™n t·∫≠p validation, t√≠nh average loss ƒë·ªÉ ƒë√°nh gi√° xem m√¥ h√¨nh h·ªçc t·ªët ƒë·∫øn ƒë√¢u sau m·ªói epoch.
# D√πng torch.no_grad() ƒë·ªÉ t·∫Øt vi·ªác t√≠nh gradient ‚Äî r·∫•t c·∫ßn thi·∫øt khi kh√¥ng training.
# D√πng .eval() ƒë·ªÉ ƒë·∫£m b·∫£o model ho·∫°t ƒë·ªông ƒë√∫ng ch·∫ø ƒë·ªô inference.
    
    # ƒê·∫∑t model v·ªÅ ch·∫ø ƒë·ªô evaluation (kh√¥ng d√πng dropout, batchnorm,...)
    model.eval()

    total_val_loss = 0  # Bi·∫øn d√πng ƒë·ªÉ c·ªông d·ªìn loss trong qu√° tr√¨nh ƒë√°nh gi√°

    # Kh√¥ng t√≠nh to√°n gradient trong khi ƒë√°nh gi√° ‚Üí ti·∫øt ki·ªám b·ªô nh·ªõ v√† tƒÉng t·ªëc
    with torch.no_grad():
        for batch in dataloader:
            # ƒê∆∞a d·ªØ li·ªáu batch l√™n GPU (ho·∫∑c CPU t√πy thi·∫øt b·ªã)
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # Forward pass: t√≠nh to√°n output v√† loss
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            
            # C·ªông d·ªìn loss v√†o t·ªïng loss
            total_val_loss += outputs.loss.item()
    
    # Tr·∫£ v·ªÅ loss trung b√¨nh tr√™n to√†n b·ªô t·∫≠p validation
    return total_val_loss / len(dataloader)

def main():
    # 6.1 Setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Ki·ªÉm tra xem GPU c√≥ s·∫µn kh√¥ng, n·∫øu kh√¥ng d√πng CPU
    print(f"\nüñ•Ô∏è ƒêang s·ª≠ d·ª•ng: {device}")
    
    # Hyperparameters
    BATCH_SIZE = 2  # S·ªë l∆∞·ª£ng m·∫´u trong m·ªói batch
    MAX_LENGTH = 64  # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa chu·ªói ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
    NUM_EPOCHS = 5  # S·ªë epoch hu·∫•n luy·ªán
    
    # Directories
    cache_dir = "model_cache"  # Th∆∞ m·ª•c cache model ƒë√£ t·∫£i
    results_dir = "training/results"  # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
    os.makedirs(results_dir, exist_ok=True)  # T·∫°o th∆∞ m·ª•c k·∫øt qu·∫£ n·∫øu ch∆∞a c√≥

    # 6.2 Model Loading
    print("\n1Ô∏è‚É£ ƒêang t·∫£i model t·ª´ cache...")
    try:
        # T·∫£i tokenizer v√† model t·ª´ cache
        tokenizer = AutoTokenizer.from_pretrained(f"{cache_dir}/tokenizer", local_files_only=True)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            f"{cache_dir}/model",  # T·∫£i model ƒë√£ hu·∫•n luy·ªán t·ª´ cache
            local_files_only=True,
            use_cache=False  # T·∫Øt s·ª≠ d·ª•ng cache khi t·∫£i model
        ).to(device)  # ƒê∆∞a model l√™n thi·∫øt b·ªã (GPU/CPU)
        print("‚úÖ T·∫£i model th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå L·ªói: {str(e)}")
        return

    # 6.3 Data Loading
    print("\n2Ô∏è‚É£ ƒêang t·∫£i d·ªØ li·ªáu...")
    try:
        # T·∫£i d·ªØ li·ªáu hu·∫•n luy·ªán t·ª´ file JSON
        with open('training/data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"‚úÖ ƒê√£ t·∫£i {len(data)} m·∫´u d·ªØ li·ªáu")
    except FileNotFoundError:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file data.json!")
        return

    # 6.4 Data Preparation
    print("\n3Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu...")
    # T·∫°o dataset t·ª´ d·ªØ li·ªáu ƒë√£ t·∫£i v√† tokenizer
    dataset = ChatbotDataset(data, tokenizer, max_length=MAX_LENGTH)
    # Chia t·∫≠p d·ªØ li·ªáu th√†nh train v√† validation
    train_size = int(0.9 * len(dataset))  # 90% cho training
    val_size = len(dataset) - train_size  # 10% cho validation
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # Dataloader cho train v√† validation
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE,  # S·ªë l∆∞·ª£ng m·∫´u trong m·ªói batch
        shuffle=True,  # Shuffle d·ªØ li·ªáu
        num_workers=2,  # S·ª≠ d·ª•ng 2 worker ƒë·ªÉ load d·ªØ li·ªáu song song
        pin_memory=True  # D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c l∆∞u trong b·ªô nh·ªõ pin ƒë·ªÉ truy c·∫≠p nhanh h∆°n
    )
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        pin_memory=True
    )

    # 6.5 Optimizer & Scheduler Setup
    # Kh·ªüi t·∫°o optimizer v√† learning rate scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)  # AdamW optimizer
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=50,  # S·ªë b∆∞·ªõc warmup
        num_training_steps=len(train_dataloader) * NUM_EPOCHS  # T·ªïng s·ªë b∆∞·ªõc hu·∫•n luy·ªán
    )

    # 6.6 Training Loop
    print(f"\n4Ô∏è‚É£ B·∫Øt ƒë·∫ßu training...")
    print(f"üìä Train: {len(train_dataset)} m·∫´u, Validation: {len(val_dataset)} m·∫´u")
    print(f"üìà Batch size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}")

    best_val_loss = float('inf')  # Kh·ªüi t·∫°o gi√° tr·ªã loss t·ªëi ∆∞u ban ƒë·∫ßu
    for epoch in range(NUM_EPOCHS):
        # Training phase
        model.train()  # Chuy·ªÉn model sang ch·∫ø ƒë·ªô training
        total_loss = 0  # Kh·ªüi t·∫°o t·ªïng loss
        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')
        
        for batch in progress_bar:
            # Forward pass (t√≠nh output t·ª´ batch)
            outputs = model(
                input_ids=batch['input_ids'].to(device),
                attention_mask=batch['attention_mask'].to(device),
                labels=batch['labels'].to(device)
            )
            
            # T√≠nh loss v√† l√†m backpropagation
            loss = outputs.loss
            total_loss += loss.item()  # C·ªông d·ªìn loss
            loss.backward()  # Backpropagation

            # Optimization step (c·∫≠p nh·∫≠t tham s·ªë c·ªßa model)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()  # C·∫≠p nh·∫≠t tham s·ªë
            scheduler.step()  # C·∫≠p nh·∫≠t learning rate
            optimizer.zero_grad()  # Reset gradient

            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})  # Hi·ªÉn th·ªã loss trong progress bar

        # Validation phase
        val_loss = evaluate_model(model, val_dataloader, device)  # ƒê√°nh gi√° loss tr√™n t·∫≠p validation
        print(f"\nüìä Epoch {epoch+1}: Train Loss = {total_loss/len(train_dataloader):.4f}, Val Loss = {val_loss:.4f}")

        # Save best model based on validation loss
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            print("üíæ L∆∞u model t·ªët nh·∫•t...")
            model.save_pretrained(f"{results_dir}/best_model")
            tokenizer.save_pretrained(f"{results_dir}/best_model")

    # 6.7 Model Testing
    print("\n5Ô∏è‚É£ Test model...")
    test_inputs = [
        "Xin ch√†o!",  # Test case 1
        "2 + 2 b·∫±ng m·∫•y?",  # Test case 2
        "H√¥m nay th·ªùi ti·∫øt th·∫ø n√†o?"  # Test case 3
    ]

    model.eval()  # Chuy·ªÉn model sang ch·∫ø ƒë·ªô inference
    for test_input in test_inputs:
        print(f"\nInput: {test_input}")
        processed_input = preprocess_input(test_input)  # Ti·ªÅn x·ª≠ l√Ω ƒë·∫ßu v√†o
        inputs = tokenizer(
            f"chat: {processed_input}", 
            return_tensors="pt",  # Chuy·ªÉn ƒë·ªïi th√†nh tensor PyTorch
            max_length=64,  # ƒê·∫∑t ƒë·ªô d√†i t·ªëi ƒëa cho chu·ªói
            padding=True,  # Padding n·∫øu c·∫ßn
            truncation=True  # C·∫Øt b·ªõt n·∫øu qu√° ƒë·ªô d√†i
        ).to(device)

        # Sinh c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=64,  # ƒê·ªô d√†i t·ªëi ƒëa cho c√¢u tr·∫£ l·ªùi
                num_beams=3,  # Beam search v·ªõi 3 beam
                early_stopping=True  # D·ª´ng s·ªõm khi ƒë√£ t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi t·ªët
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)  # Gi·∫£i m√£ c√¢u tr·∫£ l·ªùi
        print(f"Output: {response}")

    print("\n‚ú® Training ho√†n t·∫•t!")

# 7. ENTRY POINT
if __name__ == "__main__":
    main()
